I have a document and I'd like to generate an outline for it.

The goal here is to use that outline as a quick way to enable me to navigate through the contents to read it efficiently.

Even though the contents are markdown files, you don't necessarily have to align with the headers, etc.

Ignore parts such as publishing dates, references, etc. I mainly want the outline to learn what's in the document.

There might be redundant headers, or there could be ways to group certain sections that are far away in similar parts of the outline, etc.

The document to be processed has been broken down into chunks and at every iteration you have access to the current chunk and the outline we've generated so far.

Your job is to look at the contents of the current chunk and modify the outline accordingly.

INPUT FORMAT
============

OUTLINE SO FAR:
```

```

CURRENT CHUNK:
```

```

OUTPUT FORMAT
=============
The updated outline in the form

- Concept 1
    - Subconcept1
    - Subconcept2
        - Subsubconcept1
        - ...
    - ...
- Concept 2
    - ...
...

As mentioned previously, the outline isn't supposed to be a hard-fixed representative of the header structure of the markdown you're processing but rather about being able to have a global structure of the contents in the document, accounting for content overlaps between various "headers" in the markdown form.

Example of the format:

- Using Dictionary Learning Features as Classifiers
  - Introduction
    - Motivation: Extracting human interpretable features from LLMs
    - Goal: Train better classifiers on internal model representations
    - Advantages of using features:
      - Competitive performance compared to raw-activations.
      - Interpretable decision-trees.
      - Visualizing classifiers for understanding datasets.
      - Identifying spurious correlations for adversarial attacks.
    - Complexity introduced by using features
  - Classifiers using Feature Activations vs Raw Activations
    - Linear probes trained on raw activations as effective classifiers
    - Studying feature activations for classification of harmful prompts
    - Factors affecting feature-classification performance:
      - Consistent Inclusion/Exclusion of Human Assistant tags
      - Mixing domain-relevant data into SAE training
      - Max-pooling feature activations across the context
  - Evaluation Datasets
    - synthetic_1: Validation dataset from the same distribution as the training dataset.
    - synthetic_2: Synthetic data generated by a different model.
    - synthetic_3: Translated versions of synthetic_2.
    - human: Challenging questions from human experts.
  - Feature Visualization for Spurious Correlations
    - Observation of features firing on academic publication formatting.
    - Correlation between academic style and harmlessness in training data.
    - Analysis of feature activity across evaluation datasets.
    - Constructing Adversarial Examples based on Spurious Correlations
      - Academic publication formatting suffixes
      - Raw-activation based suffixes
  - Out of Distribution Generalization of Features
    - Features outperform raw-activations on highly out of distribution datasets like â€œsynthetic_3"
  - Feature Decision Trees
    - Decision trees are less performant than logistic regression
    - Feature-based classifiers outperform raw-activation equivalents in decision trees
    - Decision trees trained on feature activations are easy to interpret
  - Residuals
    - Supplementing features with the SAE residual can close the gap with raw-activations when features perform worse
  - Important Considerations for Feature-based Classifier Performance
    - Consistent handling of Human/Assistant tags
    - Mixing domain relevant data
    - Max-pooling feature activations instead of last token
    - Impact of Human/Assistant tags on classifier data
    - Training dictionaries with domain-relevant data
    - Aggregating maximum feature activation across the entire context
